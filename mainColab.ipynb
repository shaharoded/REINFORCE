{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1cbe86",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - Assignment 2\n",
    "## Training and Comparing REINFORCE Agents on CartPole-v1\n",
    "\n",
    "This notebook implements, trains, and evaluates Policy Gradient agents.\n",
    "\n",
    "### Assignment Structure:\n",
    "- **Section 1**: Vanilla REINFORCE Agent\n",
    "- **Section 2**: REINFORCE with Baseline (Value Function)\n",
    "- **Section 3**: Comparison and Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_root = notebook_dir  # Since notebook is in project root\n",
    "\n",
    "# Add to path if not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c34c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Force complete module reload\n",
    "import sys\n",
    "modules_to_remove = [key for key in list(sys.modules.keys()) \n",
    "                    if 'agent' in key.lower() or 'src' in key or 'utils' in key or 'ffnn' in key]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Import custom agents\n",
    "from src.agent import ReinforceAgent, ReinforceBaselineAgent\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# --- SEED SETTING ---\n",
    "SEED = 42\n",
    "def set_seeds(seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds()\n",
    "print(f\"✓ Seeds set to {SEED}\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea6770f",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(results: Dict[str, Any], agent_name: str, save_path: str = None):\n",
    "    \"\"\"Plot rewards and losses.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Rewards\n",
    "    rewards = results['rewards']\n",
    "    window = 100\n",
    "    axes[0].plot(rewards, alpha=0.6, label='Episode Reward')\n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        axes[0].plot(np.arange(window-1, len(rewards)), moving_avg, color='red', linewidth=2, label='100-Ep Avg')\n",
    "    \n",
    "    axes[0].axhline(y=475, color='green', linestyle='--', label='Target (475)')\n",
    "    axes[0].set_title(f'{agent_name}: Rewards')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Reward')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Loss\n",
    "    if results['loss']:\n",
    "        axes[1].plot(results['loss'], color='orange', alpha=0.6)\n",
    "        axes[1].set_title(f'{agent_name}: Loss')\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def save_results(agent, results, name, base_dir=\"models\"):\n",
    "    \"\"\"Save model weights and results.\"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(agent.policy_net.state_dict(), f\"{base_dir}/{name}_{timestamp}_policy.pth\")\n",
    "    if hasattr(agent, 'value_net'):\n",
    "        torch.save(agent.value_net.state_dict(), f\"{base_dir}/{name}_{timestamp}_value.pth\")\n",
    "        \n",
    "    # Save metrics\n",
    "    np.savez(f\"{base_dir}/{name}_{timestamp}_results.npz\", **results)\n",
    "    print(f\"✓ Saved {name} to {base_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8ab9f",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State Dim: {state_dim}, Action Dim: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63dbb2",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Vanilla REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e781cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config_reinforce = {\n",
    "    'gamma': 0.99,\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dims': [128, 128]\n",
    "}\n",
    "\n",
    "# Initialize Agent\n",
    "set_seeds(SEED) # Reset seeds for fair comparison\n",
    "agent_reinforce = ReinforceAgent(state_dim, action_dim, config_reinforce)\n",
    "\n",
    "print(\"Initialized Vanilla REINFORCE Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722239f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"Starting Training...\")\n",
    "results_reinforce = agent_reinforce.train(\n",
    "    env, \n",
    "    max_episodes=3000, \n",
    "    target_reward=475.0, \n",
    "    window=100\n",
    ")\n",
    "\n",
    "print(f\"Training finished. Converged: {results_reinforce['converged']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9354252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze\n",
    "plot_training_metrics(results_reinforce, \"Vanilla REINFORCE\", \"results/reinforce_training.png\")\n",
    "save_results(agent_reinforce, results_reinforce, \"reinforce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b8488",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (Same hyperparameters for fair comparison)\n",
    "config_baseline = {\n",
    "    'gamma': 0.99,\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dims': [128, 128]\n",
    "}\n",
    "\n",
    "# Initialize Agent\n",
    "set_seeds(SEED) # Reset seeds\n",
    "agent_baseline = ReinforceBaselineAgent(state_dim, action_dim, config_baseline)\n",
    "\n",
    "print(\"Initialized REINFORCE with Baseline Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"Starting Training...\")\n",
    "results_baseline = agent_baseline.train(\n",
    "    env, \n",
    "    max_episodes=3000, \n",
    "    target_reward=475.0, \n",
    "    window=100\n",
    ")\n",
    "\n",
    "print(f\"Training finished. Converged: {results_baseline['converged']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze\n",
    "plot_training_metrics(results_baseline, \"REINFORCE w/ Baseline\", \"results/baseline_training.png\")\n",
    "save_results(agent_baseline, results_baseline, \"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b65403",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df61f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate moving averages\n",
    "window = 100\n",
    "r_reinforce = results_reinforce['rewards']\n",
    "r_baseline = results_baseline['rewards']\n",
    "\n",
    "ma_reinforce = np.convolve(r_reinforce, np.ones(window)/window, mode='valid')\n",
    "ma_baseline = np.convolve(r_baseline, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(ma_reinforce, label='Vanilla REINFORCE', alpha=0.8)\n",
    "plt.plot(ma_baseline, label='REINFORCE w/ Baseline', alpha=0.8)\n",
    "\n",
    "plt.axhline(y=475, color='green', linestyle='--', label='Target')\n",
    "plt.title('Comparison: Moving Average Reward (Window=100)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.savefig(\"results/comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Vanilla Converged at: {results_reinforce['episodes_trained']}\")\n",
    "print(f\"Baseline Converged at: {results_baseline['episodes_trained']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
